---
layout: index
title: "COMP6258: Differentiable Programming and Deep Learning"
subtitle: "2025-26"
githubHeader: "false"
credits: Maintained by <a href="http://www.ecs.soton.ac.uk/people/jsh2">Professor Jonathon Hare</a> and <a href="http://www.ecs.soton.ac.uk/people/am8n17">Dr Antonia Marcu</a>.
---

# Deep Learning Coursework: The COMP6258 Reproducibility Challenge - Paper Ideas


| **Title**                                                                                            | **URL**                                                                                                               |
|----------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|
| Activation by Interval-wise Dropout A Simple Way to Prevent Neural Networks from Plasticity Loss | [https://arxiv.org/pdf/2502.01342](https://arxiv.org/pdf/2502.01342) |
| DESIGNING CONCISE CONVNETS WITH COLUMNAR STAGES | [https://proceedings.iclr.cc/paper_files/paper/2025/file/a12e362d89d4e0b40760f839f91550ee-Paper-Conference.pdf](https://proceedings.iclr.cc/paper_files/paper/2025/file/a12e362d89d4e0b40760f839f91550ee-Paper-Conference.pdf) |
| Improving Out-of-Distribution Detection via Dynamic Covariance Calibration | [https://openreview.net/pdf?id=UjLxG9k4B6](https://openreview.net/pdf?id=UjLxG9k4B6) |
| Collapse-Proof Non-Contrastive Self-Supervised Learning | [https://openreview.net/pdf?id=wIfl8PK6Op](https://openreview.net/pdf?id=wIfl8PK6Op) |
| ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via α-β-Divergence | [https://openreview.net/pdf?id=vt65VjJakt](https://openreview.net/pdf?id=vt65VjJakt) |
| Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations | [https://openreview.net/pdf?id=TPuFRuNano](https://openreview.net/pdf?id=TPuFRuNano) |
| Are Sparse Autoencoders Useful? A Case Study in Sparse Probing | [https://openreview.net/pdf?id=rNfzT8YkgO](https://openreview.net/pdf?id=rNfzT8YkgO) |
| AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping | [https://openreview.net/pdf?id=2hgHyoyVWj](https://openreview.net/pdf?id=2hgHyoyVWj) |
| Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning | [https://arxiv.org/pdf/2502.07560](https://arxiv.org/pdf/2502.07560) |
| NegMerge: Sign-Consensual Weight Merging for Machine Unlearning | [https://arxiv.org/pdf/2410.05583](https://arxiv.org/pdf/2410.05583) |
| Predicting the Susceptibility of Examples to Catastrophic Forgetting | [https://arxiv.org/pdf/2406.09935](https://arxiv.org/pdf/2406.09935) |
| BOOD: Boundary-based Out-Of-Distribution Data Generation | [https://arxiv.org/pdf/2508.00350](https://arxiv.org/pdf/2508.00350) |
| Weakly-Supervised Contrastive Learning for Imprecise Class Labels | [https://arxiv.org/pdf/2505.22028](https://arxiv.org/pdf/2505.22028) |
| Sum-of-Parts: Self-Attributing Neural Networks with End-to-End Learning of Feature Groups | [https://arxiv.org/pdf/2310.16316](https://arxiv.org/pdf/2310.16316) |
| Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models | [https://openreview.net/pdf?id=Zm0Kper4yx](https://openreview.net/pdf?id=Zm0Kper4yx) |
| Synergy Between the Strong and the Weak: Spiking Neural Networks are Inherently Self-Distillers| [https://openreview.net/pdf?id=BrmR69AhUg](https://openreview.net/pdf?id=BrmR69AhUg) | 
| Spiking Neural Networks Need High-Frequency Information| [https://openreview.net/pdf?id=owNPAl7LNK](https://openreview.net/pdf?id=owNPAl7LNK) |
| Pixel2Feature Attack (P2FA): Rethinking the Perturbed Space to Enhance Adversarial Transferability | [https://openreview.net/pdf?id=bPJo5uSkOJ](https://openreview.net/pdf?id=bPJo5uSkOJ) | 
| NeuronTune: Towards Self-Guided Spurious Bias Mitigation | [https://openreview.net/pdf?id=qC5FZs34Xr](https://openreview.net/pdf?id=qC5FZs34Xr) |

