---
layout: index
title: "COMP6258: Differentiable Programming and Deep Learning"
subtitle: "2024-25"
githubHeader: "false"
credits: Maintained by <a href="http://www.ecs.soton.ac.uk/people/jsh2">Professor Jonathon Hare</a> and <a href="http://www.ecs.soton.ac.uk/people/am8n17">Dr Antonia Marcu</a>.
---

# Deep Learning Coursework: The COMP6258 Reproducibility Challenge - Paper Ideas


| **URL**                                                                                            | **Title**                                                                                                               | **Notes**                                                                                                                                          |
|----------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| https://openreview.net/forum?id=6O3Q6AFUTu                                                         | NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation    | Diffusion models. How the choice of noise impacts generation                                                                                       |
| https://proceedings.mlr.press/v235/qiao24a.html                                                    | Ensemble Pruning for Out-of-distribution Generalization                                                                 | Combines pruning and ensembles for OOD                                                                                                             |
| https://proceedings.mlr.press/v235/liu24ar.html                                                    | Learning with Partial-Label and Unlabeled Data: A Uniform Treatment for Supervision Redundancy and Insufficiency        | semi-supervised/constrastive, looking at label "qualities" (and potentially loss)                                                                  |
| https://proceedings.mlr.press/v235/kim24c.html                                                     | LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging                                          | Model compression and merging; applications in embedded inference                                                                                  |
| https://proceedings.mlr.press/v235/horoi24a.html                                                   | Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis                                       | Model merging + representation "alignment"                                                                                                         |
| https://proceedings.mlr.press/v235/gong24b.html                                                    | Does Label Smoothing Help Deep Partial Label Learning?                                                                  | links to label smoothing, potentially could let you critically think about what is being trained against (and potentially loss)                    |
| https://proceedings.mlr.press/v235/balestriero24b.html                                             | How Learning by Reconstruction Produces Uninformative Features for Perception                                           | Reconstruction vs classification. Representation learning and relationships between learned representation and task (and potentially loss)         |
| https://openreview.net/pdf?id=xutrKezbPF                                                           | CIFD: Controlled Information Flow to Enhance Knowledge Distillation                                                     | Looks at a form of knowledge transfer. Explores a range of models including CLIP                                                                   |
| https://openreview.net/pdf?id=uwSaDHLlYc                                                           | Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment                           | Data distillation, creating diverse training instances. Potentially linked with notions of data quality and diversity                              |
| https://openreview.net/pdf?id=owuEcT6BTl                                                           | Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space                                          | Training dynamics and learning of "concepts" in the data                                                                                           |
| https://openreview.net/pdf?id=opt72TYzwZ                                                           | Optimal ablation for interpretability                                                                                   | Mechanistic interpretability and critically thinking about setting up a benchmark                                                                  |
| https://openreview.net/pdf?id=oNMnR0NJ2e                                                           | A Label is Worth a Thousand Images in Dataset Distillation                                                              | Dataset distillation and soft labels. Prompts critical thinking about experimental design and subfield                                             |
| https://openreview.net/pdf?id=lrSrJZZCle                                                           | CODA: A Correlation-Oriented Disentanglement and Augmentation Modeling Scheme for Better Resisting Subpopulation Shifts | Shortcut learning, OOD, disentanglement                                                                                                            |
| https://openreview.net/pdf?id=IoRT7EhFap                                                           | Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning                                           | Looks at a low-frequency bias in completion (opposite to hf bias in classification)                                                                |
| https://openreview.net/pdf?id=ebBnKVxMcZ                                                           | Confidence Calibration of Classifiers with Many Classes                                                                 | Calibration for DL models and relationship between task/target and what is learned                                                                 |
| https://openreview.net/pdf?id=96gXvFYWSE - this one I'm still not sure of the difference they draw | Pearls from Pebbles: Improved Confidence Functions for Auto-labeling                                                    | Automatically labelling training data. Relation to calibration. Critically thinking about the objective of calibration and the limitations         |
| https://openreview.net/pdf?id=8HeUvbImKT                                                           | WeiPer: OOD Detection using Weight Perturbations of Class Projections                                                   | OOD detection. Think about weight space and qualities that give good OOD detection performance                                                     |
| https://openreview.net/pdf?id=2TktDpGqNM                                                           | Overcoming Common Flaws in the Evaluation of Selective Classification Systems                                           | !!!paper could be made more DL centric. Focus on designing learning paradigms and losses to include the idea of reject                             |
| https://openreview.net/forum?id=XmyxQaTyck&noteId=1FUnA4F1N3                                       | Benchmarking the Attribution Quality of Vision Models                                                                   | Benchmarking attributions; critical thinking about experimental design & space to propose new experiments, etc                                     |
| https://openreview.net/pdf?id=cmcD05NPKa                                                           | LEARNING THE GREATEST COMMON DIVISOR: EXPLAINING TRANSFORMER PREDICTIONS                                                | Transformers for math tasks + interpretation of learned models                                                                                     |
| https://openreview.net/forum?id=zlgfRk2CQa                                                         | Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints                                     | Recurrent nets appropriately regularised can solve easy to hard problems consistently. Could be expanded, e.g. to actually explore what is learned |
| https://openreview.net/forum?id=YrAxxscKM2                                                         | Why Do We Need Weight Decay in Modern Deep Learning?                                                                    | Do we need weight decay? What is it giving us?                                                                                                     |
| https://openreview.net/forum?id=nxL7eazKBI                                                         | Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks                                           | Dissasembling and reassembling models; opportunities to ask further questions about the "qualities" of these                                       |
| https://openreview.net/forum?id=4bKEFyUHT4                                                         | Convolutional Differentiable Logic Gate Networks                                                                        | Deep nets with logic gate components (what about the gradient?!)                                                                                   |
