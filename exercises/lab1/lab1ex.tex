
\documentclass[a4paper]{article}
\usepackage[a4paper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=2cm]{geometry}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{textpos} % package for the positioning
\usepackage{tcolorbox}
\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.3em}

\begin{document}
\setlength{\leftskip}{20pt}
\title{Lab 1 Exercise - Playing with gradients and matrices in PyTorch}
\author{Jonathon Hare (jsh2@ecs.soton.ac.uk)}

\maketitle

% \begin{abstract}
% \end{abstract}
% \tableofcontents

This is the first of a series of exercises that you need to work through \textbf{on your own} after completing the accompanying lab session. You'll need to write up your results/answers/findings and submit this to ECS handin as a PDF document along with the other lab exercises near the end of the module (1 pdf document per lab). If you are asked to implement a function, please supply a code snippet in your file.

You should use \emph{no more} than one side of A4 to cover your responses to \emph{this} exercise. This exercise is worth 5\% of your overall module grade.

\section{Implement a matrix factorisation using gradient descent}\label{sgd}

You saw (briefly) in the lectures that a form of low-rank matrix factorisation can be achieved by considering the following optimisation problem:

\begin{equation}
	\min\limits_{\hat{\bm U}, \hat{\bm V}}( \| \bm A - \hat{\bm U}\hat{\bm V}^\top \|_{\rm F}^2 )
\end{equation}
where $\bm A \in \mathbb{R}^{m \times n}$, $\hat{\bm U} \in \mathbb{R}^{m \times r}$, $\hat{\bm V} \in \mathbb{R}^{n \times r}$ and $r<\min(m,n)$. The following pseudocode gives an algorithm that uses gradient descent to optimise this problem  (note that $\hat{\bm U}_{r,*}$ refers to the entire $r$-th row of $\hat{\bm U}$):

\begin{lstlisting}[mathescape=true,tabsize=4]
	Inputs: 
		$\bm A$: $\mathbb{R}^{m \times n}$ input matrix.
		$r$: rank of the factorisation.
		$N$: number of epochs.
		$\eta$: learning rate.

	Outputs:
		$\hat{\bm U}$: $\mathbb{R}^{m \times r}$ matrix initialised with values from $\mathcal{U}(0,1)$.
		$\hat{\bm V}$: $\mathbb{R}^{n \times r}$ matrix initialised with values from $\mathcal{U}(0,1)$.

	Algorithm:
		For epoch = 1 to $N$
			For r = 1 to $m$
				For c = 1 to $n$
					$e = \bm{A}_{rc} - \hat{\bm U}_{r,*} (\hat{\bm V}_{c,*})^\top$
					$\hat{\bm U}_{r,*} \leftarrow \hat{\bm U}_{r,*} + \eta \cdot e \cdot \hat{\bm V}_{c,*}$
					$\hat{\bm V}_{c,*} \leftarrow \hat{\bm V}_{c,*} + \eta \cdot e \cdot \hat{\bm U}_{r,*}$
		return $\hat{\bm U}, \hat{\bm V}$
\end{lstlisting}

\begin{tcolorbox}[title=1.1 Implement gradient-based factorisation (1 mark)]
Implement the above code method using PyTorch by completing the following method:

\begin{lstlisting}[language=Python]
from typing import Tuple

def sgd_factorise(A: torch.Tensor, rank: int, num_epochs=1000, 
	lr=0.01) -> Tuple[torch.Tensor, torch.Tensor]:
\end{lstlisting}

\end{tcolorbox}

\begin{tcolorbox}[title=1.2 Factorise and compute reconstruction error (1 mark)]
Use your code to compute the rank-2 factorisation of the following matrix and state the result (use the default values for learning rate and number of epochs):
\begin{equation*}
	\begin{bmatrix}
		0.3374 & 0.6005 & 0.1735\\
		3.3359 & 0.0492 & 1.8374\\
		2.9407 & 0.5301 & 2.2620\\
	\end{bmatrix}	
\end{equation*}
Compute the value of the reconstruction loss\footnote{Hint: you can use \texttt{torch.nn.functional.mse\_loss} with \texttt{reduction=`sum'} to do this.} $\| \bm A - \hat{\bm U}\hat{\bm V}^\top\|_{\rm F}^2$.
\end{tcolorbox}

\section{Compare your result to truncated SVD}

A truncated Singular Value Decomposition is defined as:
\begin{equation}
	\tilde{\bm A} = \bm U_t \bm \Sigma_t \bm V_t^\top
\end{equation}
where $\tilde{\bm A} \in \mathbb{R}^{m \times n}$, $\bm U \in \mathbb{R}^{m \times t}$, $\bm S \in \mathbb{R}^{t \times t}$, $\bm V \in \mathbb{R}^{m \times t}$ and $t<\min(m,n)$.
\\
\begin{tcolorbox}[title=2.1 Compare to the truncated-SVD (1 mark)]
Compute the SVD of the matrix $\bm A$ using \texttt{torch.svd}. Set the last singular value to zero, and compute the reconstruction. What is the error? How does it compare to the result from your algorithm above? Why is this the case\footnote{Maybe google Eckart-Young theorem}?
\end{tcolorbox}

\section{Matrix completion}
One of the really neat things we can do with our gradient-based approach to matrix factorisation is to perform a task called \emph{matrix completion}. Assume for a moment that you have a problem where you have a number of observations corresponding to different instances, but that these observations are both noisy and incomplete (you don't have observations of all features for all instances). You might ask if you can use unsupervised learning to \emph{impute} the missing observations, and (potentially) remove some of the noise --- this is exactly what matrix completion is about. 

Of course, to perform matrix completion, you do have to make some kind of assumption about the processes that produced the data you have. One assumption that you could make is that the process is controlled by a small number of \emph{latent variables}\footnote{you'll see many forms of latent variable model throughout this module, and also in advanced machine learning and in data-mining if you're taking them.}. Perhaps the simplest latent variable model is one based on low-rank  factorisation: the chosen rank in such a case literally controls the number of latent variables, and thus the complexity of the model. We have known about such models since at least the late 1980s (where a technique called Latent Semantic Indexing was invented), but it wasn't until 2006\footnote{\url{http://sifter.org/~simon/journal/20061211.html}} that we saw a gradient-based approach being proposed and used with a very large dataset. 

The only difference between the pseudocode in Section~\ref{sgd}, and one that works with incomplete observations is that we would only perform gradient updates where we have observations:

\begin{lstlisting}[mathescape=true,tabsize=4]
		For epoch = 1 to $N$
			For r = 1 to $m$
				For c = 1 to $n$
					if $\bm{A}_{rc}$ was observed
						$e = \bm{A}_{rc} - \hat{\bm U}_{r,*} (\hat{\bm V}_{c,*})^\top$
						$\hat{\bm U}_{r,*} \leftarrow \hat{\bm U}_{r,*} + \eta \cdot e \cdot \hat{\bm V}_{c,*}$
						$\hat{\bm V}_{c,*} \leftarrow \hat{\bm V}_{c,*} + \eta \cdot e \cdot \hat{\bm U}_{r,*}$
		return $\hat{\bm U}, \hat{\bm V}$
\end{lstlisting}

\begin{tcolorbox}[title=3.1 Implement and test masked factorisation (1 mark)]
	Create a new version of your factorisation code that additionally takes a second matrix of the same shape of the matrix being factorised. This second matrix should be a binary mask, with 1's where the value is valid, and zeros elsewhere. Use the mask to only compute updates using the valid values.

	Your implementation should have the following signature:

	\begin{lstlisting}[language=Python]
def sgd_factorise_masked(A: torch.Tensor, M: torch.Tensor, rank: int,
	num_epochs=1000, lr=0.01) -> Tuple[torch.Tensor, torch.Tensor]: 
	\end{lstlisting}

	Use your function to compute the rank-2 factorisation of the following matrix (again using default learning rate and number of epochs):

	\begin{equation*}
	\begin{bmatrix}
		0.3374 & 0.6005 & 0.1735 \\
		       & 0.0492 & 1.8374 \\
		2.9407 &        & 2.2620 \\
	\end{bmatrix}	
\end{equation*}

	What is your estimate of the completed matrix? How does this compare to the original matrix $\bm A$? What does this tell you?
\end{tcolorbox}

\section{Movie Recommendation}
Matrix completion can be used to solve real-world problems, such as building a recommender system. In fact, the approach you implemented in the previous section arose because of a competition to build a recommender system for \emph{Netflix}. The original \emph{Netflix Challenge} provided ratings of movies (on a scale of 1 to 5) given by different viewers, and the challenge was to predict how a user might rate a film that they hadn't seen before. This challenge is a specific form of recommendation called \emph{collaborative filtering}, which it turns out can be viewed as a matrix completion problem. Collaborative filtering approaches work on the basis that people often share similar tastes --- the underlying assumption being that there are a small number of latent groupings of people with similar likes and dislikes.
\\[1em]
Consider that the raw data consists of tuples (user, movie, rating). This data could be converted into a matrix where each row corresponds to a movie, each column to a user, and the elements to the rating given by a particular user to a particular movie. Clearly not all users will have watched and rated all movies, so there will be many missing entries. Matrix completion provides a way of estimating what values those missing entries should take --- essentially providing guesses for how a user would rate a movie that they haven't yet watched based on the ratings of others with similar patterns of taste.
\\[1em]
The stochastic, one-element at-a-time, algorithm you wrote in part 3, can be used to perform matrix completion for very large datasets (it does in fact mirror the basic approach that was originally developed for the Netflix Challenge), however, the python implementation is very very slow. The following code implements a vectorised gradient descent that updates all the values at once (by summing all the updates to the $\bm U$ and $\bm V$ matrices) and is faster:

\begin{lstlisting}[language=Python]
def gd_factorise_masked(A: torch.Tensor, M: torch.Tensor, rank: int, 
      num_epochs: int=1000, lr:float=1e-5) -> Tuple[torch.Tensor, torch.Tensor]:
  U = torch.rand(A.shape[0], rank)
  V = torch.rand(A.shape[1], rank)

  for e in range(num_epochs):
    err = (A - U @ V.t()) * M
    U += lr * err @ V
    V += lr * err.t() @ U

  return U, V
\end{lstlisting}

\begin{tcolorbox}[title=4.1 Predict a movie rating (1 mark)]

I've created a pytorch tensor that contains the ratings of the top-1000 movies from the top 10000 most active users within the original Netflix dataset. You can download a serialised version of this tensor from \url{http://ecs-vlc.github.io/COMP6258/labs/lab1/ratings.pt} and load it using \verb|torch.load('ratings.pt')|. I've also created a list that maps the row-index of the ratings tensor to the title of each movie, and provided that as a CSV file at \url{http://ecs-vlc.github.io/COMP6258/labs/lab1/titles.csv}. 
\\[1em]
Using the \verb|gd_factorise_masked| function (default learning rate and epochs, and a rank of 5), what rating do you predict the 5th user (column index 4) might give to the movie ``A Beautiful Mind''? What is the \emph{sum of squared errors} of the resultant matrix, computed over the valid (e.g. unmasked) values?

\end{tcolorbox}


\end{document}



